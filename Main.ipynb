{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0e0d818-4c2e-4240-91f3-1553dbdd1723",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Cleaning\n",
    "import pandas as pd\n",
    "df = pd.read_csv('bidding_data.csv')\n",
    "group_cols = [col for col in df.columns if col not in ['bid_made', 'alone', 'score', 'hand_id']]\n",
    "df_filtered = df.groupby(group_cols).filter(lambda g: len(g) > 2)\n",
    "grouped_data = df_filtered.groupby(group_cols).apply(\n",
    "    lambda x: pd.Series({\n",
    "        'bid_win_rate': (x.loc[x['bid_made'] == 1, 'score']>0).mean(),\n",
    "        'bid_expected_score': x.loc[x['bid_made'] == 1, 'score'].mean(),\n",
    "        'not_bid_win_rate': (x.loc[x['bid_made'] == 0, 'score']>0).mean(),\n",
    "        'not_bid_expected_score': x.loc[x['bid_made'] == 0, 'score'].mean(),\n",
    "        'alone_win_rate': (x.loc[x['alone'] == 1, 'score']>0).mean(),\n",
    "        'alone_expected_score': x.loc[x['alone'] == 1, 'score'].mean()\n",
    "    })\n",
    ")\n",
    "grouped_data.to_csv(\"grouped.csv\")\n",
    "mask = ~(\n",
    "    (\n",
    "\n",
    "        \n",
    "        grouped_data[\"bid_expected_score\"].isna() & grouped_data[\"not_bid_expected_score\"].between(-1, 1)\n",
    "    ) | (\n",
    "        grouped_data[\"not_bid_expected_score\"].isna() & grouped_data[\"bid_expected_score\"].between(-1, 1)\n",
    "    )\n",
    ")\n",
    "\n",
    "# Keep rows that do NOT satisfy the condition\n",
    "grouped_data_cleaned = grouped_data[mask]\n",
    "\n",
    "def should_bid(row):\n",
    "    a, b = row[\"bid_expected_score\"], row[\"not_bid_expected_score\"]\n",
    "    \n",
    "    # Case 1: both exist\n",
    "    if pd.notna(a) and pd.notna(b):\n",
    "        return int(a > b)\n",
    "    \n",
    "    # Case 2: only a exists\n",
    "    if pd.notna(a) and pd.isna(b):\n",
    "        if a > 1:\n",
    "            return 1\n",
    "        elif a < -1:\n",
    "            return 0\n",
    "    \n",
    "    # Case 3: only b exists\n",
    "    if pd.isna(a) and pd.notna(b):\n",
    "        if b < -1:\n",
    "            return 1\n",
    "        elif b > 1:\n",
    "            return 0\n",
    "grouped_data_cleaned[\"should_bid\"] = grouped_data_cleaned.apply(should_bid, axis=1)\n",
    "\n",
    "def should_alone(row):\n",
    "    a, b = row[\"bid_expected_score\"], row[\"alone_expected_score\"]\n",
    "\n",
    "    if pd.notna(b) and (b > 1.5 or b > a):\n",
    "        return 1\n",
    "\n",
    "    else:\n",
    "        return 0\n",
    "grouped_data_cleaned[\"should_alone\"] = grouped_data_cleaned.apply(should_alone, axis=1)\n",
    "\n",
    "grouped_data_cleaned.to_csv(\"grouped_cleaned.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae41bf92-1877-4989-adba-e4f0c085ac17",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "# Load data\n",
    "df = pd.read_csv('grouped_cleaned.csv')\n",
    "\n",
    "# Define features and targets\n",
    "features = ['num_trump', 'right_bower', 'left_bower', 'trump_ace', 'trump_king', \n",
    "            'trump_queen', 'trump_ten', 'trump_nine', 'offsuit_aces', 'flipped_jack', \n",
    "            'flipped_ace', 'seat_position', 'bidding_round']\n",
    "X = df[features]\n",
    "y_bid = df['should_bid']\n",
    "y_alone = df['should_alone']\n",
    "\n",
    "# Split data (same splits for both targets)\n",
    "X_train, X_test, _, _ = train_test_split(X, y_bid, test_size=0.2, random_state=42)\n",
    "_, _, y_bid_train, y_bid_test = train_test_split(X, y_bid, test_size=0.2, random_state=42)\n",
    "_, _, y_alone_train, y_alone_test = train_test_split(X, y_alone, test_size=0.2, random_state=42)\n",
    "\n",
    "# Model Pipelines ---------------------------------------------------\n",
    "\n",
    "# 1. BIDDING MODELS\n",
    "bid_logreg = Pipeline([\n",
    "    ('scaler', StandardScaler()),\n",
    "    ('logreg', LogisticRegression(max_iter=1000, random_state=42))\n",
    "])\n",
    "\n",
    "bid_rf = Pipeline([\n",
    "    ('scaler', StandardScaler()),\n",
    "    ('rf', RandomForestClassifier(random_state=42))\n",
    "])\n",
    "\n",
    "# 2. GOING ALONE MODELS\n",
    "alone_logreg = Pipeline([\n",
    "    ('scaler', StandardScaler()),\n",
    "    ('logreg', LogisticRegression(max_iter=1000, random_state=42))\n",
    "])\n",
    "\n",
    "alone_rf = Pipeline([\n",
    "    ('scaler', StandardScaler()),\n",
    "    ('rf', RandomForestClassifier(random_state=42))\n",
    "])\n",
    "\n",
    "# Training ----------------------------------------------------------\n",
    "\n",
    "# Train bidding models\n",
    "bid_logreg.fit(X_train, y_bid_train)\n",
    "bid_rf.fit(X_train, y_bid_train)\n",
    "\n",
    "# Train going-alone models\n",
    "alone_logreg.fit(X_train, y_alone_train)\n",
    "alone_rf.fit(X_train, y_alone_train)\n",
    "\n",
    "# Evaluation --------------------------------------------------------\n",
    "\n",
    "def evaluate_model(model, X_test, y_test, model_name):\n",
    "    y_pred = model.predict(X_test)\n",
    "    print(f\"\\n{model_name} Performance:\")\n",
    "    print(classification_report(y_test, y_pred))\n",
    "    print(f\"Accuracy: {accuracy_score(y_test, y_pred):.4f}\")\n",
    "    return model\n",
    "\n",
    "# Evaluate all models\n",
    "bid_logreg = evaluate_model(bid_logreg, X_test, y_bid_test, \"Bidding Logistic Regression\")\n",
    "bid_rf = evaluate_model(bid_rf, X_test, y_bid_test, \"Bidding Random Forest\")\n",
    "\n",
    "alone_logreg = evaluate_model(alone_logreg, X_test, y_alone_test, \"Going Alone Logistic Regression\")\n",
    "alone_rf = evaluate_model(alone_rf, X_test, y_alone_test, \"Going Alone Random Forest\")\n",
    "\n",
    "# Prediction Example ------------------------------------------------\n",
    "\n",
    "def make_prediction(model, input_data, model_name):\n",
    "    sample_input = pd.DataFrame([input_data], columns=features)\n",
    "    prediction = model.predict(sample_input)\n",
    "    proba = model.predict_proba(sample_input)\n",
    "    print(f\"\\n{model_name} Prediction:\")\n",
    "    print(f\"Predicted class: {prediction[0]}\")\n",
    "    print(f\"Probability estimates: {proba[0]}\")\n",
    "    return prediction\n",
    "\n",
    "# Sample input: [num_trump, right_bower, left_bower, trump_ace, trump_king, \n",
    "#                trump_queen, trump_ten, trump_nine, offsuit_aces, flipped_jack, \n",
    "#                flipped_ace, seat_position, bidding_round]\n",
    "sample_hand = [5, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0]\n",
    "\n",
    "make_prediction(bid_logreg, sample_hand, \"Bidding Logistic Regression\")\n",
    "make_prediction(bid_rf, sample_hand, \"Bidding Random Forest\")\n",
    "make_prediction(alone_logreg, sample_hand, \"Going Alone Logistic Regression\")\n",
    "make_prediction(alone_rf, sample_hand, \"Going Alone Random Forest\")\n",
    "\n",
    "# Feature Importance ------------------------------------------------\n",
    "\n",
    "print(\"\\nBidding Random Forest Feature Importance:\")\n",
    "print(pd.DataFrame({\n",
    "    'Feature': features,\n",
    "    'Importance': bid_rf.named_steps['rf'].feature_importances_\n",
    "}).sort_values('Importance', ascending=False))\n",
    "\n",
    "print(\"\\nGoing Alone Random Forest Feature Importance:\")\n",
    "print(pd.DataFrame({\n",
    "    'Feature': features,\n",
    "    'Importance': alone_rf.named_steps['rf'].feature_importances_\n",
    "}).sort_values('Importance', ascending=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3075833a-6143-412e-b375-6de1f53ee15b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test\n",
    "from importlib import reload\n",
    "from euchre import play_euchre\n",
    "\n",
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "winning_score = 2000\n",
    "\n",
    "play_euchre(winning_score, player_strategies = ['new', 'scorecard_complex', 'new', 'scorecard_complex'], verbose=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "201b0c9a-2407-415f-9553-7b0110132d95",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test\n",
    "from importlib import reload\n",
    "from euchre import play_euchre\n",
    "\n",
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "winning_score = 2000\n",
    "\n",
    "play_euchre(winning_score, player_strategies = ['new', 'scorecard_simple', 'new', 'scorecard_simple'], verbose=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "934bb34a-eed3-4db9-bbb9-bff959a2c54d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
